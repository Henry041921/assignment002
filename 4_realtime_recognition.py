import cv2
import mediapipe as mp
import numpy as np
import joblib
import time
import copy

# --- é…ç½® ---
MODEL_FILE = "gesture_model.pkl"

# 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
print(f"Loading model from {MODEL_FILE}...")
try:
    classifier = joblib.load(MODEL_FILE)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except FileNotFoundError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ 3_train_model.py")
    exit()

# 2. åˆå§‹åŒ– MediaPipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,  # è§†é¢‘æµæ¨¡å¼
    max_num_hands=1,  # åªè¯†åˆ«ä¸€åªæ‰‹
    min_detection_confidence=0.7,  # æé«˜ä¸€ç‚¹é—¨æ§›ï¼Œé˜²æ­¢ä¹±é£˜
    min_tracking_confidence=0.5
)


def normalize_landmarks(landmarks):
    """
    ğŸŒŸ æ ¸å¿ƒå…³é”®ï¼šå¿…é¡»ä¸è®­ç»ƒæ—¶çš„é¢„å¤„ç†é€»è¾‘å®Œå…¨ä¸€è‡´ï¼
    """
    temp_landmark_list = copy.deepcopy(landmarks)

    # --- 1. ç›¸å¯¹åæ ‡è½¬æ¢ ---
    base_x, base_y, base_z = 0, 0, 0
    for index, landmark_point in enumerate(temp_landmark_list):
        if index == 0:
            base_x, base_y, base_z = landmark_point[0], landmark_point[1], landmark_point[2]

        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x
        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y
        temp_landmark_list[index][2] = temp_landmark_list[index][2] - base_z

    # --- 2. å°ºåº¦å½’ä¸€åŒ– ---
    flattened = [val for sublist in temp_landmark_list for val in sublist]
    max_value = max(list(map(abs, flattened)))

    def normalize_(n):
        return n / max_value if max_value != 0 else 0

    final_features = []
    for lm in temp_landmark_list:
        final_features.extend([normalize_(lm[0]), normalize_(lm[1]), normalize_(lm[2])])

    return final_features


# 3. æ‰“å¼€æ‘„åƒå¤´
cap = cv2.VideoCapture(0)  # å¦‚æœå¤–æ¥æ‘„åƒå¤´ï¼Œå°è¯•æ”¹æˆ 1
if not cap.isOpened():
    print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
    exit()

print("\nğŸ¥ æ‘„åƒå¤´å·²å¯åŠ¨ï¼(æŒ‰ 'Q' é”®é€€å‡º)")

# FPS è®¡ç®—å˜é‡
prev_frame_time = 0
new_frame_time = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # é•œåƒç¿»è½¬ (è®©ä½ çœ‹ç€æ›´è‡ªç„¶)
    frame = cv2.flip(frame, 1)

    # è½¬ä¸º RGB ä¾› MediaPipe ä½¿ç”¨
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # æ£€æµ‹æ‰‹åŠ¿
    results = hands.process(img_rgb)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # A. ç”»éª¨æ¶
            mp_drawing.draw_landmarks(
                frame,
                hand_landmarks,
                mp_hands.HAND_CONNECTIONS
            )

            # B. æå–åŸå§‹åæ ‡
            raw_landmarks = []
            for lm in hand_landmarks.landmark:
                raw_landmarks.append([lm.x, lm.y, lm.z])

            # C. ğŸ”¥ æ‰§è¡Œå½’ä¸€åŒ– (å…³é”®ä¸€æ­¥ï¼)
            processed_features = normalize_landmarks(raw_landmarks)

            # D. AI é¢„æµ‹
            # å°† list è½¬ä¸º numpy æ•°ç»„ (å½¢çŠ¶ 1x63)
            input_data = np.array([processed_features])

            try:
                prediction = classifier.predict(input_data)
                predicted_label = prediction[0]

                # è·å–ç½®ä¿¡åº¦ (å¦‚æœæ˜¯ RF æˆ– KNN)
                if hasattr(classifier, "predict_proba"):
                    proba = classifier.predict_proba(input_data)
                    confidence = np.max(proba)
                    display_text = f"{predicted_label} ({confidence * 100:.1f}%)"
                else:
                    display_text = f"Gesture: {predicted_label}"

                # E. åœ¨å±å¹•ä¸Šæ˜¾ç¤ºç»“æœ
                cv2.rectangle(frame, (0, 0), (300, 70), (0, 0, 0), -1)  # é»‘è‰²èƒŒæ™¯æ¡
                cv2.putText(frame, display_text, (10, 50),
                            cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)

            except Exception as e:
                print(f"é¢„æµ‹å‡ºé”™: {e}")

    # æ˜¾ç¤º FPS
    new_frame_time = time.time()
    fps = 1 / (new_frame_time - prev_frame_time) if prev_frame_time != 0 else 0
    prev_frame_time = new_frame_time
    cv2.putText(frame, f"FPS: {int(fps)}", (10, frame.shape[0] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

    # æ˜¾ç¤ºç”»é¢
    cv2.imshow('ASL Recognition (High Accuracy Mode)', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
hands.close()