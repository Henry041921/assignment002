import cv2
import mediapipe as mp
import numpy as np
import joblib
import time
import copy
import os
import tkinter as tk
from tkinter import filedialog, messagebox, font

# --- é…ç½® ---
MODEL_FILE = "gesture_model.pkl"

# 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
print(f"Loading model from {MODEL_FILE}...")
try:
    classifier = joblib.load(MODEL_FILE)
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except FileNotFoundError:
    # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œå¼¹çª—æç¤ºå¹¶é€€å‡º
    root = tk.Tk()
    root.withdraw()
    messagebox.showerror("é”™è¯¯", f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_FILE}\nè¯·å…ˆè¿è¡Œ 3_train_model.py")
    exit()

# åˆå§‹åŒ– MediaPipe ç»˜å›¾å·¥å…·
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils


def normalize_landmarks(landmarks):
    """
    ğŸŒŸ æ ¸å¿ƒå…³é”®ï¼šå¿…é¡»ä¸è®­ç»ƒæ—¶çš„é¢„å¤„ç†é€»è¾‘å®Œå…¨ä¸€è‡´ï¼
    """
    temp_landmark_list = copy.deepcopy(landmarks)

    # --- 1. ç›¸å¯¹åæ ‡è½¬æ¢ ---
    base_x, base_y, base_z = 0, 0, 0
    for index, landmark_point in enumerate(temp_landmark_list):
        if index == 0:
            base_x, base_y, base_z = landmark_point[0], landmark_point[1], landmark_point[2]

        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x
        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y
        temp_landmark_list[index][2] = temp_landmark_list[index][2] - base_z

    # --- 2. å°ºåº¦å½’ä¸€åŒ– ---
    flattened = [val for sublist in temp_landmark_list for val in sublist]
    max_value = max(list(map(abs, flattened)))

    def normalize_(n):
        return n / max_value if max_value != 0 else 0

    final_features = []
    for lm in temp_landmark_list:
        final_features.extend([normalize_(lm[0]), normalize_(lm[1]), normalize_(lm[2])])

    return final_features


def predict_and_draw(image, hands_module):
    """
    é€šç”¨å¤„ç†å‡½æ•°ï¼šæ¥æ”¶ä¸€å¼ å›¾ç‰‡ï¼ˆæˆ–è§†é¢‘å¸§ï¼‰ï¼Œè¿›è¡Œæ£€æµ‹ã€é¢„æµ‹å¹¶ç»˜åˆ¶ç»“æœã€‚
    """
    # è½¬ä¸º RGB ä¾› MediaPipe ä½¿ç”¨
    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # æ£€æµ‹æ‰‹åŠ¿
    results = hands_module.process(img_rgb)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # A. ç”»éª¨æ¶
            mp_drawing.draw_landmarks(
                image,
                hand_landmarks,
                mp_hands.HAND_CONNECTIONS
            )

            # B. æå–åŸå§‹åæ ‡
            raw_landmarks = []
            for lm in hand_landmarks.landmark:
                raw_landmarks.append([lm.x, lm.y, lm.z])

            # C. ğŸ”¥ æ‰§è¡Œå½’ä¸€åŒ–
            processed_features = normalize_landmarks(raw_landmarks)

            # D. AI é¢„æµ‹
            input_data = np.array([processed_features])

            try:
                prediction = classifier.predict(input_data)
                predicted_label = prediction[0]

                # è·å–ç½®ä¿¡åº¦
                if hasattr(classifier, "predict_proba"):
                    proba = classifier.predict_proba(input_data)
                    confidence = np.max(proba)
                    display_text = f"{predicted_label} ({confidence * 100:.1f}%)"
                else:
                    display_text = f"Gesture: {predicted_label}"

                # E. åœ¨å±å¹•ä¸Šæ˜¾ç¤ºç»“æœ
                # è·å–æ–‡å­—å¤§å°ä»¥ä¾¿åŠ¨æ€è°ƒæ•´èƒŒæ™¯æ¡†
                (text_w, text_h), _ = cv2.getTextSize(display_text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)
                cv2.rectangle(image, (0, 0), (text_w + 20, text_h + 40), (0, 0, 0), -1)
                cv2.putText(image, display_text, (10, text_h + 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)

            except Exception as e:
                print(f"é¢„æµ‹å‡ºé”™: {e}")

    return image


def run_camera_mode():
    """æ¨¡å¼ 1: å®æ—¶æ‘„åƒå¤´è¯†åˆ«"""
    print("\nğŸš€ æ­£åœ¨å¯åŠ¨æ‘„åƒå¤´...")

    # è§†é¢‘æ¨¡å¼ä¸‹ static_image_mode=False æ›´å¿«
    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=1,
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5
    )

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼")
        return

    prev_frame_time = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # é•œåƒç¿»è½¬
        frame = cv2.flip(frame, 1)

        # æ ¸å¿ƒå¤„ç†
        frame = predict_and_draw(frame, hands)

        # æ˜¾ç¤º FPS
        new_frame_time = time.time()
        fps = 1 / (new_frame_time - prev_frame_time) if prev_frame_time != 0 else 0
        prev_frame_time = new_frame_time
        cv2.putText(frame, f"FPS: {int(fps)}", (10, frame.shape[0] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

        # æ˜¾ç¤ºæç¤ºä¿¡æ¯
        cv2.putText(frame, "Press 'Q' to Exit", (frame.shape[1] - 200, frame.shape[0] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

        cv2.imshow('ASL Recognition (Camera Mode)', frame)

        # æŒ‰ 'q' é€€å‡ºå¾ªç¯
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    hands.close()
    print("ğŸ¥ æ‘„åƒå¤´æ¨¡å¼å·²ç»“æŸ")


def run_image_mode():
    """æ¨¡å¼ 2: å•å¼ å›¾ç‰‡è¯†åˆ« (æ–‡ä»¶é€‰æ‹©å™¨)"""

    # æ‰“å¼€æ–‡ä»¶é€‰æ‹©å¯¹è¯æ¡†
    file_path = filedialog.askopenfilename(
        title="é€‰æ‹©ä¸€å¼ æ‰‹åŠ¿å›¾ç‰‡",
        filetypes=[("Image files", "*.jpg *.jpeg *.png *.bmp *.webp")]
    )

    if not file_path:
        print("å–æ¶ˆé€‰æ‹©")
        return

    # å›¾ç‰‡æ¨¡å¼ä¸‹ static_image_mode=True ç²¾åº¦æ›´é«˜
    hands = mp_hands.Hands(
        static_image_mode=True,
        max_num_hands=1,
        min_detection_confidence=0.5
    )

    frame = cv2.imread(file_path)
    if frame is None:
        messagebox.showerror("é”™è¯¯", "æ— æ³•è¯»å–å›¾ç‰‡ï¼Œè¯·ç¡®ä¿æ–‡ä»¶æœªæŸåã€‚")
        hands.close()
        return

    print(f"ğŸ–¼ï¸ æ­£åœ¨åˆ†æ: {file_path} ...")

    # æ ¸å¿ƒå¤„ç†
    frame = predict_and_draw(frame, hands)

    # æ˜¾ç¤ºæç¤º
    cv2.putText(frame, "Press Any Key to Close", (10, frame.shape[0] - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    # æ˜¾ç¤ºç»“æœ
    window_name = f'Result: {os.path.basename(file_path)}'
    cv2.imshow(window_name, frame)

    # ç­‰å¾…ä»»æ„é”®å…³é—­
    cv2.waitKey(0)
    cv2.destroyAllWindows()
    hands.close()


# ==========================================
# ğŸ–¥ï¸ GUI ä¸»ç•Œé¢é€»è¾‘
# ==========================================
def start_gui_app():
    # åˆ›å»ºä¸»çª—å£
    root = tk.Tk()
    root.title("ASL æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ")
    root.geometry("400x350")

    # è®¾ç½®å±…ä¸­
    screen_width = root.winfo_screenwidth()
    screen_height = root.winfo_screenheight()
    x = (screen_width - 400) // 2
    y = (screen_height - 350) // 2
    root.geometry(f"400x350+{x}+{y}")

    # å­—ä½“è®¾ç½®
    title_font = font.Font(family="Helvetica", size=16, weight="bold")
    btn_font = font.Font(family="Helvetica", size=12)

    # æ ‡é¢˜æ ‡ç­¾
    lbl_title = tk.Label(root, text="ğŸ–ï¸ ASL Gesture Recognition", font=title_font, pady=20)
    lbl_title.pack()

    # è¯´æ˜æ ‡ç­¾
    lbl_desc = tk.Label(root, text="è¯·é€‰æ‹©è¯†åˆ«æ¨¡å¼ï¼š", font=("Arial", 10), fg="gray")
    lbl_desc.pack(pady=5)

    # --- æŒ‰é’®åŒºåŸŸ ---
    # æ‘„åƒå¤´æŒ‰é’®
    btn_cam = tk.Button(root, text="ğŸ“¹ å¯åŠ¨æ‘„åƒå¤´ (Real-time)",
                        font=btn_font, bg="#e1f5fe", height=2, width=30,
                        command=run_camera_mode)  # ç‚¹å‡»è°ƒç”¨ run_camera_mode
    btn_cam.pack(pady=10)

    # å›¾ç‰‡æŒ‰é’®
    btn_img = tk.Button(root, text="ğŸ–¼ï¸ ä¸Šä¼ å›¾ç‰‡è¯†åˆ« (Upload Image)",
                        font=btn_font, bg="#fce4ec", height=2, width=30,
                        command=run_image_mode)  # ç‚¹å‡»è°ƒç”¨ run_image_mode
    btn_img.pack(pady=10)

    # é€€å‡ºæŒ‰é’®
    btn_exit = tk.Button(root, text="âŒ é€€å‡ºç¨‹åº (Exit)",
                         font=btn_font, height=1, width=30,
                         command=root.quit)
    btn_exit.pack(pady=20)

    # åº•éƒ¨ç‰ˆæƒ
    lbl_footer = tk.Label(root, text="Powered by MediaPipe & Scikit-Learn", font=("Arial", 8), fg="#ccc")
    lbl_footer.pack(side=tk.BOTTOM, pady=5)

    # å¯åŠ¨ GUI å¾ªç¯
    root.mainloop()


if __name__ == "__main__":
    start_gui_app()