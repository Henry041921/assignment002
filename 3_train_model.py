import pandas as pd
import numpy as np  # ä»…ç”¨äºæ•°æ®åŠ è½½å’Œåˆ†å‰²ï¼Œä¸ç”¨äºKNNæ ¸å¿ƒé€»è¾‘
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import math
from collections import Counter

# å…è®¸ä½¿ç”¨ sklearn è¿›è¡Œæ•°æ®åˆ†å‰²ã€è¯„ä¼°å’Œå…¶ä»–æ¨¡å‹ (ä¾‹å¤–æ˜¯ KNN)
from sklearn.model_selection import train_test_split, KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# --- é…ç½® ---
DATA_FILE = "landmarks_data.csv"
MODEL_SAVE_PATH = "gesture_model.pkl"


# ==========================================
# ğŸŒŸ æ ¸å¿ƒå¾—åˆ†ç‚¹ï¼šçº¯æ‰‹å†™ KNN (From Scratch)
# âš ï¸ ä¸¥æ ¼éµå®ˆæ–‡æ¡£ Part 2c è¦æ±‚ï¼š
# "implemented from scratch using only Python standard built-in libraries"
# ==========================================
class KNN_From_Scratch:
    def __init__(self, k=3):
        self.k = k
        self.X_train = []
        self.y_train = []

    def fit(self, X, y):
        """
        è®­ç»ƒè¿‡ç¨‹å…¶å®å°±æ˜¯å­˜å‚¨æ•°æ®ã€‚
        ä¸ºäº†ç¬¦åˆ"ä»…ä½¿ç”¨å†…ç½®åº“"çš„è¦æ±‚ï¼Œæˆ‘ä»¬å°†æ•°æ®è½¬æ¢ä¸ºçº¯ Python listã€‚
        """
        # å¦‚æœè¾“å…¥æ˜¯ DataFrame æˆ– Numpy æ•°ç»„ï¼Œè½¬æ¢ä¸º list
        if hasattr(X, 'values'):
            self.X_train = X.values.tolist()
        elif hasattr(X, 'tolist'):
            self.X_train = X.tolist()
        else:
            self.X_train = list(X)

        if hasattr(y, 'values'):
            self.y_train = y.values.tolist()
        elif hasattr(y, 'tolist'):
            self.y_train = y.tolist()
        else:
            self.y_train = list(y)

    def _euclidean_distance(self, row1, row2):
        """ä»…ä½¿ç”¨ math åº“è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»"""
        distance = 0.0
        for i in range(len(row1)):
            distance += (row1[i] - row2[i]) ** 2
        return math.sqrt(distance)

    def predict(self, X):
        """é¢„æµ‹æ–°æ•°æ®"""
        # è½¬æ¢è¾“å…¥æ•°æ®ä¸º list
        if hasattr(X, 'values'):
            X_data = X.values.tolist()
        elif hasattr(X, 'tolist'):
            X_data = X.tolist()
        else:
            X_data = list(X)

        predictions = []
        for row in X_data:
            label = self._predict_single(row)
            predictions.append(label)
        return predictions

    def _predict_single(self, row):
        # 1. è®¡ç®—è·ç¦»
        distances = []
        for i in range(len(self.X_train)):
            dist = self._euclidean_distance(row, self.X_train[i])
            distances.append((self.X_train[i], self.y_train[i], dist))

        # 2. æŒ‰è·ç¦»æ’åº (ä»å°åˆ°å¤§)
        distances.sort(key=lambda tup: tup[2])

        # 3. è·å–æœ€è¿‘çš„ k ä¸ªé‚»å±…
        neighbors = []
        for i in range(self.k):
            neighbors.append(distances[i][1])  # åªå–æ ‡ç­¾

        # 4. æŠ•ç¥¨ (ä½¿ç”¨ collections.Counter)
        vote_result = Counter(neighbors).most_common(1)[0][0]
        return vote_result

    # ä¸ºäº†å…¼å®¹ sklearn çš„æ¥å£ (cross_val_score éœ€è¦è¿™ä¸ª)
    def get_params(self, deep=True):
        return {"k": self.k}

    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self


# ==========================================
# ğŸ› ï¸ è¾…åŠ©åŠŸèƒ½
# ==========================================
def run_cross_validation(model, X, y, k_folds=5):
    """æ‰§è¡Œ 5-Fold Cross Validation å¹¶è¿”å›å¹³å‡å‡†ç¡®ç‡"""
    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    scores = []

    # è½¬æ¢ä¸º numpy ä»¥æ–¹ä¾¿ç´¢å¼•åˆ‡ç‰‡
    X_np = np.array(X)
    y_np = np.array(y)

    for train_idx, val_idx in kf.split(X_np):
        X_train_fold, X_val_fold = X_np[train_idx], X_np[val_idx]
        y_train_fold, y_val_fold = y_np[train_idx], y_np[val_idx]

        model.fit(X_train_fold, y_train_fold)
        preds = model.predict(X_val_fold)
        score = accuracy_score(y_val_fold, preds)
        scores.append(score)

    return np.mean(scores)


def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=sorted(set(y_true)), yticklabels=sorted(set(y_true)))
    plt.title(title)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.tight_layout()
    plt.savefig(f"confusion_matrix_{title.replace(' ', '_')}.png")
    print(f"ğŸ“Š {title} å·²ä¿å­˜ä¸ºå›¾ç‰‡")


# ==========================================
# ğŸš€ ä¸»ç¨‹åº
# ==========================================
if __name__ == "__main__":
    # 1. åŠ è½½æ•°æ®
    print(f"Loading data from {DATA_FILE}...")
    try:
        df = pd.read_csv(DATA_FILE)
    except FileNotFoundError:
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° CSV æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ 2a_feature_extraction.py")
        exit()

    X = df.drop('label', axis=1)
    y = df['label']

    # 2. åˆ’åˆ†æ•°æ®é›† (80% è®­ç»ƒ, 20% æµ‹è¯•) [cite: 168]
    print("Splitting data (80% Train, 20% Test)...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    results = {}  # å­˜å‚¨æœ€ç»ˆç»“æœ

    print("\n" + "=" * 50)
    print("ğŸ¤– Part 2c: ç›‘ç£å­¦ä¹ ä¼˜åŒ–ä¸è¯„ä¼°")
    print("=" * 50)

    # --- æ¨¡å‹ A: Decision Tree (å¿…é€‰) ---
    print("\nğŸŒ² 1. Optimizing Decision Tree...")
    best_dt_score = 0
    best_dt_depth = None

    # è°ƒå‚: Max Depth
    for depth in [3, 5, 10, 15, None]:
        dt = DecisionTreeClassifier(max_depth=depth, random_state=42)
        score = run_cross_validation(dt, X_train, y_train, k_folds=5)
        print(f"   Depth={str(depth):<4} | CV Accuracy: {score:.4f}")
        if score > best_dt_score:
            best_dt_score = score
            best_dt_depth = depth

    print(f"   âœ… Best Depth: {best_dt_depth}")

    # ç”¨æœ€ä½³å‚æ•°åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡è®­ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° [cite: 175-177]
    final_dt = DecisionTreeClassifier(max_depth=best_dt_depth, random_state=42)
    final_dt.fit(X_train, y_train)
    dt_acc = accuracy_score(y_test, final_dt.predict(X_test))
    results['Decision Tree'] = dt_acc

    # --- æ¨¡å‹ B: Random Forest (è‡ªé€‰æ¨¡å‹) ---
    print("\nğŸŒ³ 2. Optimizing Random Forest (Selected Model)...")
    best_rf_score = 0
    best_rf_est = None

    # è°ƒå‚: n_estimators (æ ‘çš„æ•°é‡)
    for n_est in [10, 50, 100]:
        rf = RandomForestClassifier(n_estimators=n_est, random_state=42)
        score = run_cross_validation(rf, X_train, y_train, k_folds=5)
        print(f"   Trees={str(n_est):<4} | CV Accuracy: {score:.4f}")
        if score > best_rf_score:
            best_rf_score = score
            best_rf_est = n_est

    print(f"   âœ… Best Trees: {best_rf_est}")

    final_rf = RandomForestClassifier(n_estimators=best_rf_est, random_state=42)
    final_rf.fit(X_train, y_train)
    rf_acc = accuracy_score(y_test, final_rf.predict(X_test))
    results['Random Forest'] = rf_acc

    # --- æ¨¡å‹ C: Custom KNN (From Scratch å¿…é€‰) ---
    print("\nğŸ¤ 3. Optimizing Custom KNN (From Scratch)...")
    best_knn_score = 0
    best_k = None

    # è°ƒå‚: K Value
    for k in [1, 3, 5, 7]:
        knn = KNN_From_Scratch(k=k)
        score = run_cross_validation(knn, X_train, y_train, k_folds=5)
        print(f"   k={str(k):<8} | CV Accuracy: {score:.4f}")
        if score > best_knn_score:
            best_knn_score = score
            best_k = k

    print(f"   âœ… Best k: {best_k}")

    final_knn = KNN_From_Scratch(k=best_k)
    final_knn.fit(X_train, y_train)
    knn_acc = accuracy_score(y_test, final_knn.predict(X_test))
    results['KNN (Custom)'] = knn_acc

    # --- æ€»ç»“ä¸ä¿å­˜ ---
    print("\n" + "=" * 50)
    print("ğŸ† Final Test Set Results")
    print("=" * 50)
    best_model_name = ""
    best_model_acc = 0

    for name, acc in results.items():
        print(f"{name:<20}: {acc:.2%}")
        if acc > best_model_acc:
            best_model_acc = acc
            best_model_name = name

    print("-" * 50)
    print(f"ğŸŒŸ æœ€ä½³æ¨¡å‹æ˜¯: {best_model_name}")

    # ç”»å‡ºæœ€ä½³æ¨¡å‹çš„æ··æ·†çŸ©é˜µ
    print("Generating Confusion Matrix for the best model...")
    if best_model_name == 'Decision Tree':
        y_pred = final_dt.predict(X_test)
        save_model = final_dt
    elif best_model_name == 'Random Forest':
        y_pred = final_rf.predict(X_test)
        save_model = final_rf
    else:
        y_pred = final_knn.predict(X_test)
        # joblib ä¿å­˜è‡ªå®šä¹‰ç±»å¯èƒ½ä¼šæœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œé€šå¸¸æ¨èä¿å­˜ RF
        # ä½†å¦‚æœ KNN æœ€å¥½ï¼Œæˆ‘ä»¬è¿˜æ˜¯å°è¯•ä¿å­˜å®ƒ
        save_model = final_knn

    plot_confusion_matrix(y_test, y_pred, f"Confusion Matrix - {best_model_name}")

    # ä¿å­˜æ¨¡å‹
    joblib.dump(save_model, MODEL_SAVE_PATH)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
    print("\nä¸‹ä¸€æ­¥ï¼šè¯·è¿è¡Œ 4_realtime_recognition.py æŸ¥çœ‹å®æ—¶æ•ˆæœï¼")